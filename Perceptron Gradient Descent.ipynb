{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "# seed random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Backpropagation, simple example from the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    return 1/(1+np.exp(-v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(v):\n",
    "    return sigmoid(v)*(1-sigmoid(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passing a single input through the network and calculating error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [0.045]\n",
      "Output:  [0.51125]\n",
      "Error:  [0.07556]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "\n",
    "l = 0.05 #learning rate\n",
    "x=np.array([0.5,0.1])\n",
    "y = 0.9\n",
    "w = np.array([[0.15],[-0.3]])\n",
    "b = 0\n",
    "\n",
    "#Passing the example through the network\n",
    "in_ = x@w + b\n",
    "out = sigmoid(in_)\n",
    "\n",
    "print('Input: ', in_)\n",
    "print('Output: ', out)\n",
    "\n",
    "#Calulating error\n",
    "error = 0.5*np.power(y-out,2)\n",
    "print('Error: ', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating partial derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dEdOut:  [-0.38875]\n",
      "dOutdIn:  [0.24987]\n",
      "dIndW1:  0.5\n",
      "dIndW2:  0.1\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent with Backpropagation, simple example from the lecture\n",
    "np.set_printoptions(precision=5)\n",
    "l = 0.05 #learning rate\n",
    "x=np.array([0.5,0.1])\n",
    "y = 0.9\n",
    "w = np.array([[0.15],[-0.3]])\n",
    "b = 0\n",
    "\n",
    "#Passing the example through the network\n",
    "in_ = x@w + b\n",
    "out = sigmoid(in_)\n",
    "\n",
    "#Calulating error\n",
    "error = 0.5*np.power(y-out,2)\n",
    "\n",
    "#Calculating partial derivatives\n",
    "dEdOut = out - y\n",
    "dOutdIn = sigmoid_derivative(in_)\n",
    "dIndW1 = x[0]\n",
    "dIndW2 = x[1]\n",
    "dIndB = 1\n",
    "\n",
    "print('dEdOut: ', dEdOut)\n",
    "print('dOutdIn: ', dOutdIn)\n",
    "print('dIndW1: ', dIndW1)\n",
    "print('dIndW2: ', dIndW2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation: Calculating Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dEdW1:  [-0.04857]\n",
      "dEdW2:  [-0.00971]\n",
      "dEdB:  [-0.09714]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent with Backpropagation, simple example from the lecture\n",
    "np.set_printoptions(precision=5)\n",
    "l = 0.05 #learning rate\n",
    "x=np.array([0.5,0.1])\n",
    "y = 0.9\n",
    "w = np.array([[0.15],[-0.3]])\n",
    "b = 0\n",
    "\n",
    "#Passing the example through the network\n",
    "in_ = x@w + b\n",
    "out = sigmoid(in_)\n",
    "\n",
    "#Calulating error\n",
    "error = 0.5*np.power(y-out,2)\n",
    "\n",
    "#Calculating partial derivatives\n",
    "dEdOut = out - y\n",
    "dOutdIn = sigmoid_derivative(in_)\n",
    "dIndW1 = x[0]\n",
    "dIndW2 = x[1]\n",
    "dIndB = 1\n",
    "\n",
    "#Calculating gradients/backpropagation\n",
    "dEdW1 = dEdOut*dOutdIn*dIndW1\n",
    "dEdW2 = dEdOut*dOutdIn*dIndW2\n",
    "dEdB = dEdOut*dOutdIn*dIndB\n",
    "\n",
    "print('dEdW1: ', dEdW1)\n",
    "print('dEdW2: ', dEdW2)\n",
    "print('dEdB: ', dEdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent: Updating parameters of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 =  [0.15243]\n",
      "w2 =  [-0.29951]\n",
      "b =  [0.00486]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent with Backpropagation, simple example from the lecture\n",
    "np.set_printoptions(precision=5)\n",
    "l = 0.05 #learning rate\n",
    "x=np.array([0.5,0.1])\n",
    "y = 0.9\n",
    "w = np.array([[0.15],[-0.3]])\n",
    "b = 0\n",
    "\n",
    "#Passing the example through the network\n",
    "in_ = x@w + b\n",
    "out = sigmoid(in_)\n",
    "\n",
    "#Calulating error\n",
    "error = 0.5*np.power(y-out,2)\n",
    "\n",
    "#Calculating partial derivatives\n",
    "dEdOut = out - y\n",
    "dOutdIn = sigmoid_derivative(in_)\n",
    "dIndW1 = x[0]\n",
    "dIndW2 = x[1]\n",
    "dIndB = 1\n",
    "\n",
    "#Calculating gradients/backpropagation\n",
    "dEdW1 = dEdOut*dOutdIn*dIndW1\n",
    "dEdW2 = dEdOut*dOutdIn*dIndW2\n",
    "dEdB = dEdOut*dOutdIn*dIndB\n",
    "\n",
    "#Updating parameters\n",
    "w[0] = w[0] - l* dEdW1\n",
    "w[1] = w[1] - l* dEdW2\n",
    "b = b - l* dEdB\n",
    "\n",
    "print('w1 = ', w[0])\n",
    "print('w2 = ', w[1])\n",
    "print('b = ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix notation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04857]\n",
      " [-0.00971]]\n",
      "[-0.09714]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent with Backpropagation, simple example from the lecture\n",
    "np.set_printoptions(precision=5)\n",
    "l = 0.05 #learning rate\n",
    "x=np.array([0.5,0.1])\n",
    "y = 0.9\n",
    "w = np.array([[0.15],[-0.3]])\n",
    "b = 0\n",
    "\n",
    "#Passing the example through the network\n",
    "in_ = x@w + b\n",
    "out = sigmoid(in_)\n",
    "\n",
    "#Calulating error\n",
    "error = 0.5*np.power(y-out,2)\n",
    "\n",
    "#Calculating partial derivatives - matrix notation\n",
    "dEdOut = out - y\n",
    "dOutdIn = sigmoid_derivative(in_)\n",
    "dIndW = x\n",
    "dIndB = 1\n",
    "\n",
    "#Matrix notation\n",
    "dEdW = x.reshape([len(x),1])*dEdOut*dOutdIn\n",
    "dEdB = dEdOut*dOutdIn*dIndB\n",
    "\n",
    "#Updating parameters\n",
    "w = w - l*dEdW\n",
    "b = b - l* dEdB\n",
    "\n",
    "print(dEdW)\n",
    "print(dEdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (100, 2)\n",
      "y.shape: (100,)\n",
      "[0.32276 0.39129 0.02366 0.42037 0.29666 0.01224 0.33582 0.22183 0.13878\n",
      " 0.22321 0.06433 0.33642 0.25884 0.31426 0.09194 0.53826 0.26779 0.51274\n",
      " 0.21551 0.07303 0.09603 0.5409  0.2428  0.01251 0.30088 0.32565 0.23311\n",
      " 0.37082 0.03979 0.17671 0.10083 0.35578 0.1892  0.70879 0.08183 0.31787\n",
      " 0.02324 0.17285 0.49951 0.12807 0.3561  0.00157 0.2372  0.29016 0.0651\n",
      " 0.36162 0.02155 0.21984 0.18231 0.56569 0.51658 0.21467 0.00243 0.41947\n",
      " 0.04843 0.28867 0.30489 0.15673 0.41711 0.05501 0.19028 0.09314 0.01974\n",
      " 0.23027 0.12081 0.06365 0.17914 0.58763 0.04623 0.35102 0.09351 0.36981\n",
      " 0.7152  1.      0.11697 0.12186 0.00588 0.0652  0.03183 0.11888 0.14047\n",
      " 0.13801 0.37693 0.11726 0.18447 0.22876 0.04379 0.09764 0.15827 0.05284\n",
      " 0.0823  0.19482 0.15841 0.02946 0.2519  0.13752 0.21464 0.01277 0.28875\n",
      " 0.20878]\n"
     ]
    }
   ],
   "source": [
    "#Read data from a text file\n",
    "from sklearn import preprocessing\n",
    "data = np.loadtxt(\"/Users/3049848/Dropbox/Python Files/Jupyter Files/CSC4007/2022_23/Datasets/simpleregression.txt\")\n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "#scaling the input data\n",
    "#X = preprocessing.MinMaxScaler().fit_transform(X)\n",
    "\n",
    "print (\"X.shape:\", X.shape)\n",
    "print (\"y.shape:\", y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before training:  0.15662897876245127\n",
      "Error after training:  0.15606568471922683\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#initialising parameters\n",
    "w = np.random.uniform(-1,1,[X.shape[1],1])\n",
    "b = np.random.uniform(1)\n",
    "l = 0.05\n",
    "\n",
    "y=y.reshape(len(y),1)\n",
    "\n",
    "predictions = sigmoid(X@w + b)\n",
    "error = (0.5*np.power(predictions - y,2)).mean()\n",
    "print('Error before training: ', error)\n",
    "\n",
    "\n",
    "#forward pass\n",
    "in_ = X@w+b\n",
    "out = sigmoid(in_)\n",
    "\n",
    "#backpropagation\n",
    "dEdOut = out - y\n",
    "dOutdIn = sigmoid_derivative(in_) \n",
    "dIndW = X\n",
    "dEdW = (1/X.shape[0])*(dIndW.T@(dEdOut*dOutdIn))\n",
    "dEdB = (1/X.shape[0])*np.ones([1,len(X)])@(dEdOut*dOutdIn)\n",
    "\n",
    "#updating weights\n",
    "w -= l*dEdW\n",
    "b -= l*dEdB\n",
    "\n",
    "predictions = sigmoid(X@w + b)\n",
    "error = (0.5*np.power(predictions - y,2)).mean()\n",
    "print('Error after training: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#initialising parameters\n",
    "w = np.random.uniform(-1,1,[X.shape[1],1])\n",
    "b = np.random.uniform(1)\n",
    "l = 0.05\n",
    "\n",
    "for i in range(len(X)):\n",
    "    #forward pass\n",
    "    in_ = X[i]@w+b\n",
    "    out = sigmoid(in_)\n",
    "    \n",
    "    #error calculation\n",
    "    error = (0.5)*(np.power((out-y[i]),2))\n",
    "    \n",
    "    #backpropagation\n",
    "    dEdOut = out - y[i]\n",
    "    dOutdIn = sigmoid_derivative(X[i]@w + b)\n",
    "    dEdIn = dEdOut*dEdOut\n",
    "    dIndW = X[i]\n",
    "    dEdW = (dIndW.reshape(len(dIndW),1))*dEdIn #transposing 1D arrya, we can only use x.T for 2D or higher array\n",
    "    dEdB = dEdIn*1\n",
    "    \n",
    "    #updating parameters of the model\n",
    "    w = w - l*dEdW\n",
    "    b = b - l*dEdB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before training:  0.15662897876245127\n",
      "Error after training:  0.05485830213775867\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#initialising parameters\n",
    "w = np.random.uniform(-1,1,[X.shape[1],1])\n",
    "b = np.random.uniform(1)\n",
    "l = 0.05\n",
    "\n",
    "predictions = sigmoid(X@w + b)\n",
    "error = (0.5*np.power(predictions - y,2)).mean()\n",
    "print('Error before training: ', error)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    #forward pass\n",
    "    in_ = X[i]@w+b\n",
    "    out = sigmoid(in_)\n",
    "    \n",
    "    #error calculation\n",
    "    error = (0.5)*(np.power((out-y[i]),2))\n",
    "    \n",
    "    #backpropagation\n",
    "    dEdOut = out - y[i]\n",
    "    dOutdIn = sigmoid_derivative(X[i]@w + b)\n",
    "    dEdIn = dEdOut*dEdOut\n",
    "    dIndW = X[i]\n",
    "    dEdW = (dIndW.reshape(len(dIndW),1))*dEdIn #transposing 1D arrya, we can only use x.T for 2D or higher array\n",
    "    dEdB = dEdIn*1\n",
    "    \n",
    "    #updating parameters of the model\n",
    "    w = w - l*dEdW\n",
    "    b = b - l*dEdB\n",
    "    \n",
    "predictions = sigmoid(X@w + b)\n",
    "error = (0.5*np.power(predictions - y,2)).mean()\n",
    "print('Error after training: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
